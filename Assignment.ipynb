{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kuXIQeLCcSZK",
        "outputId": "5a0bfbb0-0c54-4d9c-f0e4-9eff7ff9d00c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting groq\n",
            "  Downloading groq-0.31.1-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from groq) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from groq) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from groq) (0.28.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from groq) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from groq) (1.3.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.10 in /usr/local/lib/python3.12/dist-packages (from groq) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->groq) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->groq) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->groq) (0.4.1)\n",
            "Downloading groq-0.31.1-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: groq\n",
            "Successfully installed groq-0.31.1\n"
          ]
        }
      ],
      "source": [
        "!pip install groq  # Install official Groq Python client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"GROQ_API_KEY\"] = \"groq api key\""
      ],
      "metadata": {
        "id": "JL7MPezNdIWM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from groq import Groq\n",
        "\n",
        "# Initialize Groq client\n",
        "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
        "\n",
        "# Define a simple conversation message\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": \"Explain the importance of fast language models\"}\n",
        "]\n",
        "\n",
        "# Call Groq API for chat completion\n",
        "response = client.chat.completions.create(\n",
        "    model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "# Print the assistant reply\n",
        "print(\"Groq API reply:\\n\", response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Y9-0kKcdNXP",
        "outputId": "6c023a24-85a8-42b5-a538-838df985c2e0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Groq API reply:\n",
            " Fast language models have become increasingly important in recent years due to their wide range of applications and the growing demand for efficient and scalable natural language processing (NLP) systems. Here are some reasons why fast language models are crucial:\n",
            "\n",
            "1. **Improved User Experience**: Fast language models enable faster inference and response times, which is essential for applications that require real-time or near-real-time interactions, such as:\n",
            "\t* Virtual assistants (e.g., Siri, Alexa, Google Assistant)\n",
            "\t* Chatbots and customer service platforms\n",
            "\t* Language translation systems\n",
            "\t* Sentiment analysis and opinion mining tools\n",
            "2. **Scalability**: As the amount of text data grows exponentially, fast language models can handle larger volumes of data and scale more efficiently, making them suitable for:\n",
            "\t* Large-scale text analysis and mining\n",
            "\t* Social media monitoring and analysis\n",
            "\t* Web search and information retrieval\n",
            "3. **Cost-Effectiveness**: Fast language models reduce the computational resources required for inference, which leads to:\n",
            "\t* Lower costs for cloud computing and infrastructure\n",
            "\t* Reduced energy consumption and carbon footprint\n",
            "\t* Increased deployment possibilities on edge devices (e.g., smartphones, smart home devices)\n",
            "4. **Real-World Applications**: Fast language models enable the development of more sophisticated and practical NLP applications, such as:\n",
            "\t* Real-time language translation for travelers or diplomats\n",
            "\t* Voice-controlled interfaces for smart homes or cars\n",
            "\t* Automated content generation and summarization\n",
            "5. **Competitive Advantage**: In many industries, the ability to process and respond to text-based input quickly can provide a competitive advantage, such as:\n",
            "\t* Faster customer support response times\n",
            "\t* Improved search engine rankings and results\n",
            "\t* Enhanced user engagement and retention\n",
            "6. **Advancements in NLP Research**: Fast language models facilitate research in NLP by enabling:\n",
            "\t* Faster experimentation and prototyping\n",
            "\t* Larger-scale studies and analyses\n",
            "\t* More efficient evaluation and comparison of models\n",
            "\n",
            "To achieve fast language models, researchers and developers have been exploring various techniques, including:\n",
            "\n",
            "1. **Model pruning and distillation**: Reducing the size and complexity of models while preserving performance.\n",
            "2. **Efficient architectures**: Designing models with faster inference times, such as transformer-based models.\n",
            "3. **Quantization and knowledge distillation**: Representing model weights and activations using fewer bits, reducing computational costs.\n",
            "4. **Parallelization and distributed computing**: Distributing model computations across multiple devices or machines.\n",
            "\n",
            "The importance of fast language models will continue to grow as NLP becomes increasingly pervasive in various industries and applications.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from groq import Groq\n",
        "\n",
        "# Initialize Groq client once\n",
        "client = Groq(api_key=os.getenv(\"GROQ_API_KEY\"))\n",
        "\n",
        "def truncate_by_turns(history, n):\n",
        "    return history[-n:]\n",
        "\n",
        "def truncate_by_char_limit(history, char_limit):\n",
        "    total_chars = 0\n",
        "    truncated = []\n",
        "    for msg in reversed(history):\n",
        "        size = len(msg['content'])\n",
        "        if total_chars + size > char_limit:\n",
        "            break\n",
        "        truncated.append(msg)\n",
        "        total_chars += size\n",
        "    return list(reversed(truncated))\n",
        "\n",
        "class ConversationManager:\n",
        "    def __init__(self, summarize_every_k=3):\n",
        "        self.history = []\n",
        "        self.run_count = 0\n",
        "        self.k = summarize_every_k\n",
        "\n",
        "    def add_user_message(self, text):\n",
        "        self.history.append({\"role\": \"user\", \"content\": text})\n",
        "\n",
        "    def add_assistant_message(self, text):\n",
        "        self.history.append({\"role\": \"assistant\", \"content\": text})\n",
        "\n",
        "    def get_conversation_text(self):\n",
        "        return \"\\n\".join([f\"{msg['role']}: {msg['content']}\" for msg in self.history])\n",
        "\n",
        "    def summarize_history(self):\n",
        "        prompt = f\"Please provide a concise summary of this conversation:\\n{self.get_conversation_text()}\"\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}]\n",
        "        )\n",
        "        summary = response.choices[0].message.content.strip()\n",
        "        return summary\n",
        "\n",
        "    def generate_reply(self, trunc_type=None, trunc_limit=None):\n",
        "        self.run_count += 1\n",
        "\n",
        "        # Truncate if specified\n",
        "        history_to_send = self.history\n",
        "        if trunc_type == \"turns\" and trunc_limit:\n",
        "            history_to_send = truncate_by_turns(self.history, trunc_limit)\n",
        "        elif trunc_type == \"chars\" and trunc_limit:\n",
        "            history_to_send = truncate_by_char_limit(self.history, trunc_limit)\n",
        "\n",
        "        # Call Groq API for assistant reply\n",
        "        response = client.chat.completions.create(\n",
        "            model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "            messages=history_to_send\n",
        "        )\n",
        "        reply = response.choices[0].message.content\n",
        "        self.add_assistant_message(reply)\n",
        "\n",
        "        # Perform summarization every k-th run\n",
        "        if self.run_count % self.k == 0:\n",
        "            summary = self.summarize_history()\n",
        "            # Replace conversation with summary as a single assistant message\n",
        "            self.history = [{\"role\": \"assistant\", \"content\": f\"[Summary]\\n{summary}\"}]\n",
        "\n",
        "        return reply\n",
        "\n",
        "    def print_history(self):\n",
        "        print(\"Conversation History:\")\n",
        "        for msg in self.history:\n",
        "            print(f\"{msg['role']}: {msg['content']}\")\n",
        "\n",
        "\n",
        "conv_manager = ConversationManager()\n",
        "\n",
        "# Add multiple user messages to simulate conversation\n",
        "conv_manager.add_user_message(\"Hello, can you tell me about Groq API?\")\n",
        "reply = conv_manager.generate_reply()\n",
        "print(\"Assistant reply:\", reply)\n",
        "\n",
        "conv_manager.add_user_message(\"What are some best practices for using Groq API?\")\n",
        "reply = conv_manager.generate_reply(trunc_type=\"turns\", trunc_limit=3)  # Keep last 3 turns only\n",
        "print(\"Assistant reply with truncation by turns:\", reply)\n",
        "\n",
        "conv_manager.add_user_message(\"How can I securely store API keys when using Groq in my projects?\")\n",
        "reply = conv_manager.generate_reply(trunc_type=\"chars\", trunc_limit=500)  # Limit message content to 500 chars\n",
        "print(\"Assistant reply with truncation by chars:\", reply)\n",
        "\n",
        "# Print all conversation history\n",
        "conv_manager.print_history()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Psdl71r9ec1g",
        "outputId": "cb78f7dc-8165-487b-e1da-413d8fb74232"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant reply: Groq API! That's an interesting one.\n",
            "\n",
            "Groq is a cloud-based API platform that provides access to a range of AI models, primarily focused on natural language processing (NLP) and computer vision. The Groq API allows developers to integrate these AI capabilities into their applications, making it easier to build intelligent and automated systems.\n",
            "\n",
            "Here are some key features and facts about the Groq API:\n",
            "\n",
            "**Key Features:**\n",
            "\n",
            "1. **AI Model Library:** Groq offers a library of pre-trained AI models for various tasks, such as text classification, sentiment analysis, entity extraction, and image recognition.\n",
            "2. **API-driven:** The Groq API provides a simple and RESTful interface to access these AI models, allowing developers to integrate them into their applications.\n",
            "3. **Scalability:** Groq's cloud-based infrastructure ensures scalability and high availability, making it suitable for applications with high traffic or large data volumes.\n",
            "\n",
            "**Supported AI Models:**\n",
            "\n",
            "1. **Natural Language Processing (NLP):** Groq offers a range of NLP models for tasks like text classification, sentiment analysis, entity extraction, and language translation.\n",
            "2. **Computer Vision:** Groq provides models for image recognition, object detection, and image classification.\n",
            "\n",
            "**Benefits:**\n",
            "\n",
            "1. **Easy Integration:** The Groq API provides a straightforward way to integrate AI capabilities into applications, reducing the complexity and cost of building and maintaining in-house AI models.\n",
            "2. **Access to Advanced AI Models:** Groq's API provides access to a range of advanced AI models, which can be used to improve the accuracy and efficiency of applications.\n",
            "\n",
            "**Use Cases:**\n",
            "\n",
            "1. **Chatbots and Virtual Assistants:** Groq's NLP models can be used to build intelligent chatbots and virtual assistants.\n",
            "2. **Content Analysis:** Groq's AI models can be used to analyze and categorize text and image content.\n",
            "3. **Automated Workflows:** Groq's API can be used to automate workflows and business processes by integrating AI capabilities into applications.\n",
            "\n",
            "**Pricing:**\n",
            "\n",
            "Groq's pricing model varies depending on the specific use case, data volume, and API usage. They offer a free tier, as well as custom pricing plans for enterprises.\n",
            "\n",
            "**Getting Started:**\n",
            "\n",
            "To get started with the Groq API, you can:\n",
            "\n",
            "1. **Sign up for a free account:** Create an account on the Groq website to access the API dashboard and documentation.\n",
            "2. **Explore the API:** Review the API documentation and explore the available AI models.\n",
            "3. **Make API requests:** Start making API requests using your preferred programming language.\n",
            "\n",
            "Keep in mind that this information might not be exhaustive, and I encourage you to visit the Groq website and documentation for the most up-to-date information on their API offerings.\n",
            "\n",
            "Do you have any specific questions about the Groq API or would you like more information on a particular aspect of it?\n",
            "Assistant reply with truncation by turns: Here are some best practices for using the Groq API:\n",
            "\n",
            "**API Request Best Practices:**\n",
            "\n",
            "1. **Use the correct API endpoint:** Make sure to use the correct API endpoint for the specific task you're trying to accomplish. Groq provides a range of API endpoints for different tasks, such as text classification, sentiment analysis, and image recognition.\n",
            "2. **Provide required parameters:** Ensure that you provide all required parameters for the API endpoint you're using. Required parameters are usually marked as such in the API documentation.\n",
            "3. **Use the correct data format:** Verify that you're sending data in the correct format. For example, if an API endpoint expects JSON data, make sure to send it in JSON format.\n",
            "\n",
            "**Error Handling Best Practices:**\n",
            "\n",
            "1. **Handle errors:** Always handle errors and exceptions when making API requests. Groq's API returns error responses in a standard format, which includes an error message and a status code.\n",
            "2. **Check the status code:** Verify the status code returned by the API to determine if the request was successful. A status code of 200 indicates a successful request, while a non-200 status code indicates an error.\n",
            "\n",
            "**Performance Optimization Best Practices:**\n",
            "\n",
            "1. **Batch API requests:** If you need to make multiple API requests, consider batching them together to reduce the number of requests and improve performance.\n",
            "2. **Use caching:** Implement caching to store frequently accessed data and reduce the number of API requests.\n",
            "3. **Optimize API request payload:** Minimize the size of your API request payload to reduce latency and improve performance.\n",
            "\n",
            "**Security Best Practices:**\n",
            "\n",
            "1. **Use authentication:** Always authenticate your API requests using a valid API key or token.\n",
            "2. **Keep API keys secure:** Store your API keys securely and avoid sharing them with unauthorized parties.\n",
            "3. **Validate user input:** Always validate user input to prevent malicious data from being sent to the Groq API.\n",
            "\n",
            "**Monitoring and Logging Best Practices:**\n",
            "\n",
            "1. **Monitor API usage:** Track your API usage and monitor performance metrics to ensure you're staying within usage limits and optimizing performance.\n",
            "2. **Log API requests:** Log API requests and responses to troubleshoot issues and analyze performance.\n",
            "\n",
            "**Testing and Validation Best Practices:**\n",
            "\n",
            "1. **Test API requests:** Thoroughly test your API requests to ensure they're working correctly and returning expected results.\n",
            "2. **Validate API responses:** Validate API responses to ensure they're in the expected format and contain the expected data.\n",
            "\n",
            "**Additional Tips:**\n",
            "\n",
            "1. **Read the documentation:** Always read the Groq API documentation to understand the available API endpoints, parameters, and response formats.\n",
            "2. **Stay up-to-date:** Keep an eye on Groq's API updates and changes to ensure you're using the latest features and best practices.\n",
            "\n",
            "By following these best practices, you can ensure a smooth and efficient experience when using the Groq API.\n",
            "\n",
            "Here are some code snippets demonstrating best practices:\n",
            "```python\n",
            "import requests\n",
            "\n",
            "# Best practice: Handle errors\n",
            "try:\n",
            "    response = requests.get('https://api.groq.io/text/classify', \n",
            "                             params={'text': 'This is a sample text', \n",
            "                                      'model_id': 'my_model'})\n",
            "    response.raise_for_status()  # Raise an exception for bad status codes\n",
            "except requests.RequestException as e:\n",
            "    print(f\"Request failed: {e}\")\n",
            "\n",
            "# Best practice: Validate API responses\n",
            "if response.status_code == 200:\n",
            "    data = response.json()\n",
            "    if 'results' in data:\n",
            "        print(data['results'])\n",
            "    else:\n",
            "        print(\"Invalid response format\")\n",
            "else:\n",
            "    print(f\"Request failed with status code {response.status_code}\")\n",
            "```\n",
            "\n",
            "```javascript\n",
            "const axios = require('axios');\n",
            "\n",
            "// Best practice: Handle errors\n",
            "axios.get('https://api.groq.io/text/classify', {\n",
            "  params: {\n",
            "    text: 'This is a sample text',\n",
            "    model_id: 'my_model'\n",
            "  }\n",
            "})\n",
            ".then(response => {\n",
            "  if (response.status === 200) {\n",
            "    const data = response.data;\n",
            "    if ('results' in data) {\n",
            "      console.log(data.results);\n",
            "    } else {\n",
            "      console.log(\"Invalid response format\");\n",
            "    }\n",
            "  } else {\n",
            "    console.log(`Request failed with status code ${response.status}`);\n",
            "  }\n",
            "})\n",
            ".catch(error => {\n",
            "  console.log(`Request failed: ${error}`);\n",
            "});\n",
            "```\n",
            "Assistant reply with truncation by chars: **Securely Storing API Keys with Groq**\n",
            "\n",
            "When working with Groq, it's essential to store your API keys securely to prevent unauthorized access and protect your project. Here are some best practices to help you store your API keys securely:\n",
            "\n",
            "### 1. **Environment Variables**\n",
            "\n",
            "Store your API keys as environment variables. This approach keeps your keys out of your codebase and allows you to easily switch between different environments (e.g., development, production).\n",
            "\n",
            "*   In **Linux/Mac**, use the `export` command:\n",
            "    ```bash\n",
            "export GROQ_API_KEY=\"YOUR_API_KEY_HERE\"\n",
            "```\n",
            "*   In **Windows**, use the `set` command:\n",
            "    ```cmd\n",
            "set GROQ_API_KEY=YOUR_API_KEY_HERE\n",
            "```\n",
            "*   In **Python**, use the `python-dotenv` library:\n",
            "    ```python\n",
            "import os\n",
            "from dotenv import load_dotenv\n",
            "\n",
            "load_dotenv()\n",
            "\n",
            "api_key = os.getenv('GROQ_API_KEY')\n",
            "```\n",
            "\n",
            "### 2. **Secure Configuration Files**\n",
            "\n",
            "Store your API keys in secure configuration files. Use a secrets manager or a encrypted file storage solution.\n",
            "\n",
            "*   **JSON Web Key (JWK) files**: Store your API keys in a JWK file, which is a JSON file that contains a cryptographic key.\n",
            "*   **YAML or JSON configuration files**: Store your API keys in a YAML or JSON file, and encrypt the file using a tool like `openssl`.\n",
            "\n",
            "### 3. **Secret Management Services**\n",
            "\n",
            "Use a secret management service to securely store and retrieve your API keys.\n",
            "\n",
            "*   **HashiCorp's Vault**: A popular secret management tool that provides secure storage and retrieval of sensitive data.\n",
            "*   **AWS Secrets Manager**: A fully managed service that securely stores and retrieves sensitive data.\n",
            "*   **Google Cloud Secret Manager**: A secure way to store and manage sensitive data.\n",
            "\n",
            "### 4. **Groq's Built-in Security Features**\n",
            "\n",
            "Groq provides built-in security features to help protect your API keys.\n",
            "\n",
            "*   **Groq API Key Management**: Use Groq's API key management features to create, manage, and rotate your API keys.\n",
            "\n",
            "### Example Code (Python)\n",
            "\n",
            "Here's an example of how to securely store and use your Groq API key in Python:\n",
            "\n",
            "```python\n",
            "import os\n",
            "from dotenv import load_dotenv\n",
            "import groq\n",
            "\n",
            "# Load environment variables from .env file\n",
            "load_dotenv()\n",
            "\n",
            "# Get the API key from the environment variable\n",
            "api_key = os.getenv('GROQ_API_KEY')\n",
            "\n",
            "# Initialize the Groq client with the API key\n",
            "client = groq.Client(api_key=api_key)\n",
            "\n",
            "# Use the client to make API requests\n",
            "response = client.chat.completions.create(\n",
            "    messages=[\n",
            "        {\"role\": \"user\", \"content\": \"Explain the importance of low latency LLMs\"},\n",
            "    ],\n",
            "    model=\"llama3-8b-8192\",\n",
            ")\n",
            "\n",
            "print(response.choices[0].message.content)\n",
            "```\n",
            "\n",
            "By following these best practices and using Groq's built-in security features, you can securely store your API keys and protect your project.\n",
            "Conversation History:\n",
            "assistant: [Summary]\n",
            "Certainly! I'll provide a comprehensive guide on securely storing API keys when using Groq in your projects.\n",
            "\n",
            "**Why Secure API Keys?**\n",
            "\n",
            "API keys are sensitive information that grants access to your Groq account and projects. If compromised, they can lead to unauthorized access, data breaches, and financial losses. Therefore, it's crucial to store them securely.\n",
            "\n",
            "**Best Practices for Storing API Keys**\n",
            "\n",
            "1. **Environment Variables**: Store API keys as environment variables. This approach keeps keys out of your codebase and allows easy switching between environments (e.g., development, production).\n",
            "2. **Secure Configuration Files**: Store API keys in secure configuration files, such as JSON Web Key (JWK) files or encrypted YAML/JSON files.\n",
            "3. **Secret Management Services**: Utilize secret management services like HashiCorp's Vault, AWS Secrets Manager, or Google Cloud Secret Manager to securely store and retrieve API keys.\n",
            "\n",
            "**Example: Storing API Keys as Environment Variables**\n",
            "\n",
            "In **Linux/Mac**, use the `export` command:\n",
            "```bash\n",
            "export GROQ_API_KEY=\"YOUR_API_KEY_HERE\"\n",
            "```\n",
            "In **Windows**, use the `set` command:\n",
            "```cmd\n",
            "set GROQ_API_KEY=YOUR_API_KEY_HERE\n",
            "```\n",
            "In **Python**, use the `python-dotenv` library:\n",
            "```python\n",
            "import os\n",
            "from dotenv import load_dotenv\n",
            "\n",
            "load_dotenv()\n",
            "\n",
            "api_key = os.getenv('GROQ_API_KEY')\n",
            "```\n",
            "**Example: Using a Secure Configuration File**\n",
            "\n",
            "Store your API key in a YAML file:\n",
            "```yml\n",
            "groq_api_key: YOUR_API_KEY_HERE\n",
            "```\n",
            "Encrypt the file using `openssl`:\n",
            "```bash\n",
            "openssl enc -aes-256-cbc -in config.yaml -out config.yaml.enc\n",
            "```\n",
            "**Groq's Built-in Security Features**\n",
            "\n",
            "Groq provides built-in security features, such as:\n",
            "\n",
            "* **Groq API Key Management**: Create, manage, and rotate API keys.\n",
            "* **Role-Based Access Control (RBAC)**: Control access to projects and resources.\n",
            "\n",
            "**Additional Tips**\n",
            "\n",
            "* Limit access to API keys to only those who need it.\n",
            "* Rotate API keys regularly.\n",
            "* Monitor API key usage and detect suspicious activity.\n",
            "\n",
            "By following these best practices and utilizing Groq's built-in security features, you can ensure the secure storage and management of your API keys.\n",
            "\n",
            "Here is some sample code demonstrating secure API key storage:\n",
            "```python\n",
            "import os\n",
            "import groq\n",
            "\n",
            "# Load API key from environment variable\n",
            "api_key = os.getenv('GROQ_API_KEY')\n",
            "\n",
            "# Initialize Groq client with API key\n",
            "client = groq.Client(api_key=api_key)\n",
            "\n",
            "# Use client to make API requests\n",
            "response = client.chat.completions.create(\n",
            " messages=[\n",
            " {\"role\": \"user\", \"content\": \"Explain the importance of low latency LLMs\"},\n",
            " ],\n",
            " model=\"llama3-8b-8192\",\n",
            ")\n",
            "\n",
            "print(response.choices[0].message.content)\n",
            "```\n",
            "In this example, the API key is loaded from an environment variable, and the Groq client is initialized with the API key. This approach ensures that the API key is not hardcoded in the codebase.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "user_info_schema = {\n",
        "    \"type\": \"object\",\n",
        "    \"properties\": {\n",
        "        \"name\": {\"type\": \"string\"},\n",
        "        \"email\": {\"type\": \"string\", \"format\": \"email\"},\n",
        "        \"phone\": {\"type\": \"string\"},\n",
        "        \"location\": {\"type\": \"string\"},\n",
        "        \"age\": {\"type\": \"string\"} # Changed from \"integer\" to \"string\"\n",
        "    },\n",
        "    \"required\": [\"name\", \"email\", \"phone\", \"location\", \"age\"],\n",
        "    \"additionalProperties\": False\n",
        "}"
      ],
      "metadata": {
        "id": "akvTKsfOjj-p"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_user_info(text):\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"meta-llama/llama-4-scout-17b-16e-instruct\",\n",
        "        messages=[{\"role\": \"user\", \"content\": text}],\n",
        "        functions=[{\n",
        "            \"name\": \"extract_user_info\",\n",
        "            \"description\": \"Extract user name, email, phone, location, and age from chat\",\n",
        "            \"parameters\": user_info_schema\n",
        "        }],\n",
        "        function_call={\"name\": \"extract_user_info\"}\n",
        "    )\n",
        "    # The API returns function call data as JSON string\n",
        "    func_call = response.choices[0].message.function_call\n",
        "    extracted_json_str = func_call.arguments if func_call and hasattr(func_call, 'arguments') else \"{}\"\n",
        "    import json\n",
        "    try:\n",
        "        extracted_data = json.loads(extracted_json_str)\n",
        "    except json.JSONDecodeError:\n",
        "        extracted_data = {}\n",
        "    return extracted_data"
      ],
      "metadata": {
        "id": "LtcBdF73jlEI"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples = [\n",
        "    \"Hi, I'm John Doe, my email is john@example.com, phone 555-1234, living in New York, and I am 30 years old.\",\n",
        "    \"Hello, my name is Alice, email alice@example.com, phone number 123-456-7890, based in London. Age 28.\",\n",
        "    \"This is Bob Smith. Contact: bobsmith@email.com, phone 9876543210. Location: Toronto. Age: 35.\"\n",
        "]\n",
        "\n",
        "for i, sample in enumerate(samples, 1):\n",
        "    info = extract_user_info(sample)\n",
        "    print(f\"Sample {i} extracted info:\", info)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FuwMuzWcjp3d",
        "outputId": "8d13839e-8df3-49af-d712-6ec28d0c70a6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1 extracted info: {'age': '30', 'email': 'john@example.com', 'location': 'New York', 'name': 'John Doe', 'phone': '555-1234'}\n",
            "Sample 2 extracted info: {'age': '28', 'email': 'alice@example.com', 'location': 'London', 'name': 'Alice', 'phone': '123-456-7890'}\n",
            "Sample 3 extracted info: {'age': '35', 'email': 'bobsmith@email.com', 'location': 'Toronto', 'name': 'Bob Smith', 'phone': '9876543210'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_extracted_info(data):\n",
        "    required_fields = ['name', 'email', 'phone', 'location', 'age']\n",
        "    validated = {}\n",
        "    for field in required_fields:\n",
        "        value = data.get(field)\n",
        "        if not value:\n",
        "            validated[field] = None\n",
        "        else:\n",
        "            if field == 'age':\n",
        "                try:\n",
        "                    validated[field] = int(value)\n",
        "                except:\n",
        "                    validated[field] = None\n",
        "            else:\n",
        "                validated[field] = value\n",
        "    return validated\n"
      ],
      "metadata": {
        "id": "IAi3uXUZkdfg"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, sample in enumerate(samples, 1):\n",
        "    raw_data = extract_user_info(sample)\n",
        "    clean_data = validate_extracted_info(raw_data)\n",
        "    print(f\"Sample {i} cleaned info:\", clean_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OSDM4zhUo6AC",
        "outputId": "4d5234cf-e40c-42b7-9e07-bda6339ed5f6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample 1 cleaned info: {'name': 'John Doe', 'email': 'john@example.com', 'phone': '555-1234', 'location': 'New York', 'age': 30}\n",
            "Sample 2 cleaned info: {'name': 'Alice', 'email': 'alice@example.com', 'phone': '123-456-7890', 'location': 'London', 'age': 28}\n",
            "Sample 3 cleaned info: {'name': 'Bob Smith', 'email': 'bobsmith@email.com', 'phone': '9876543210', 'location': 'Toronto', 'age': 35}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Okay, here's a brief summary of the steps you've taken in this notebook:\n",
        "\n",
        "Installed the Groq library: You started by installing the necessary Python library for interacting with the Groq API using !pip install groq. Set up the Groq API Key: You then set your Groq API key as an environment variable using os.environ[\"GROQ_API_KEY\"] = \"...\" for secure access. Used Groq API for a simple chat completion: You initialized the Groq client and made a basic chat completion call to understand the importance of fast language models. Developed a Conversation Manager: You created a ConversationManager class to handle conversation history, including methods for adding user and assistant messages, retrieving conversation text, summarizing history using the Groq API, and generating replies with optional truncation. You tested this manager with a few turns of conversation. Defined a JSON Schema for User Information: You defined a schema (user_info_schema) to specify the structure and data types for extracting user information (name, email, phone, location, age). Initially, you set 'age' as an integer, but later corrected it to 'string' based on the model's output. Created a Function for Information Extraction: You wrote an extract_user_info function that uses the Groq API with the defined schema and function calling capabilities to extract user information from a given text. You also handled potential errors during JSON parsing. Implemented a Validation Function: You created a validate_extracted_info function to clean and validate the extracted data, specifically converting the 'age' back to an integer if possible and setting fields to None if missing. Tested Extraction and Validation: You tested the extract_user_info and validate_extracted_info functions on a list of sample text inputs (samples), demonstrating how to extract and clean the user information from each sample. In essence, you've set up the Groq API, explored its basic chat capabilities, built a system for managing conversational history with summarization, and developed a process for extracting and validating structured information from text using Groq's function calling feature.\n",
        "\n"
      ],
      "metadata": {
        "id": "jOzRiFSbrDZG"
      }
    }
  ]
}